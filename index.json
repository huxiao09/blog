[{"content":"Simulation Budget Allocation for Further Enhancing the Efficiency of Ordinal Optimization OCBA最出名的一篇文章。\n文章概述 为了用尽量少的计算量得到尽可能高的对齐概率（挑选出最有可能是真实性能最好的design），提出了OCBA分配规则。\n主要内容 Intro 在序优化的基础上发展的，个人认为OCBA能结合进序优化的部分是在序优化中crude model挑选最好的s个design和在selected set中用detailed model挑选最优的design的时候 计算量分配有意义应该有一个前提，就是在模型中评估某个design的性能会受到噪声影响，导致每次评估不一样 Intuitive上来讲，为了挑选一个好的design，更多的计算量应该分配给那些能够帮助我们识别出好的design的那些designs，即应该分配给那些更能帮助我们减少估计方差的designs，一些不重要的、不能帮助我们识别出好的design的那些designs就不要给太多计算量 Method 对于DEDS，主要的优化问题是：\n$$\\min _{\\theta_i \\in \\Theta} J\\left(\\theta_i\\right) \\equiv E\\left[L\\left(\\theta_i, \\xi\\right)\\right]$$\n最常见的肯定是用均值估计期望，即$\\bar{J}_i \\equiv \\frac{1}{N_i} \\sum_{j=1}^{N_i} L\\left(\\theta_i, \\xi_{i j}\\right)$，这里$N_i$就是分配给$\\theta_i$的计算量，但这种估计误差的收敛速度不会快于$O(1/\\sqrt N)$。假设$L(,)$是正态分布的。\n对齐概率：$P{CS}=P{\\bar{J}最小的b实际上J_b就是最小的}$。我们想做的是用尽可能少的计算量达到尽可能高的对其概率：\n$$\\begin{array}{rl} \\max _{N_1, \\ldots, N_k} \u0026amp; P{C S} \\ \\text { s.t. } \u0026amp; N_1+N_2+\\cdots+N_k=T,,N_i \\in N, i=1, \\ldots, k \\end{array}$$\n采用贝叶斯模型，把对齐概率写成后验分布的形式： $$ P{CS}=P{J\\left(\\theta_b\\right)\u0026lt;J\\left(\\theta_i\\right), i \\neq b \\mid L\\left(\\theta_i, \\xi_{i j}\\right), j=1, \\ldots, N_i, i=1,2, \\ldots, k}=P{\\tilde{J}_b\u0026lt;\\tilde{J}_i,i \\neq b} $$ 从贝叶斯学派的角度，对$J(\\theta_i)$取正态先验，则其后验分布也即也是正态分布：$\\tilde{J}_i \\sim N\\left(\\bar{J}_i, \\frac{\\sigma_i^2}{N_i}\\right)$，后验正态分布的均值和方差都能通过样本估计，则PCS可以通过蒙特卡洛仿真的方式算出来，但是蒙特卡洛太耗时，所以用Bonferroni不等式把原max问题的目标函数放小，这样相当于是最大化原目标函数的下界：\n$$P{C S}=P{\\bigcap_{i=1, i \\neq b}^k\\left(\\tilde{J}_b-\\tilde{J}_i\u0026lt;0\\right)} \\geq 1-\\sum_{i=1, i \\neq b}^k\\left[1-P{\\tilde{J}_b-\\tilde{J}_i\u0026lt;0}\\right]=1-\\sum_{i=1, i \\neq b}^k P{\\tilde{J}_b\u0026gt;\\tilde{J}_i}=APCS$$\n这样原问题就变成了（注意其实不是等价的变换）：\n$$\\begin{array}{rl} \\max _{N_1, \\ldots, N_k} APCS \\quad \\text { s.t. } N_1+N_2+\\cdots+N_k=T,,N_i \\in N, i=1, \\ldots, k \\end{array}$$\n接下来，把$N_i$放成连续实数，经过变量代换可以写出来拉格朗日函数： $$ F=1-\\sum_{\\substack{i=1 \\ i \\neq b}}^k \\int_{-\\frac{\\delta_{b, i}}{\\sigma_{b, i}}}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{\\frac{t^2}{2}} d t-\\lambda\\left(\\sum_{i=1}^k N_i-T\\right) $$ 然后写KKT条件（还好都是变量不是泛函，所以写的比较清晰，注意这里的$\\sigma$和$\\delta$都是视为和$N_i,N_b$无关的）。我们其实是想找到$N_i,N_j,N_b$之间的关系，经过一系列的近似（$N_b\u0026raquo;N_i,T\\rightarrow \\infty$）和推导可以得到下面这个定理：\n直觉上来讲这个定理给的准则(1)(2)都看起来是合理的，对于不是$b$的design，方差越大就分配越多的计算量，样本均值比$b$的大的越多则分配越少的计算量；其他designs的方差越大则相对给$b$分配的计算就越少。然后就有了OCBA的分配法则：\n在每个step，通过前面step分配的计算量可以算出$\\sigma$和$\\delta$，然后依据定理一对这一step的计算量进行分配。\nExperiments 对比对象：平均分配、贪婪分配、CCY、Two-Stage Rinott Procedure\n数值实验：$L(,)$正态分布、均匀分布、具有大方差的正态分布、Flat and Steep case、更多的design、Buffer分配问题。\nOCBA表现的基本上都很好，即使是总的计算量不大的情况下（即使OCBA这套准则是在计算量趋于无穷的情况下才推导出来的）\nReviews 【优点】\n非常实用的一套方法，推导都比较清晰，而且是祖师爷级别的文章了，养活了很多人，即使现在都还有不少人关注，好像一般最牛逼的、开创性的一些文章推导都不是很复杂 做了比较充分的数值实验，考虑到了大方差、不同噪声的分布的情况，OCBA表现得都很好 【缺点】\nPCS转到APCS，最大化原问题的下界，其实这儿没有说明这样做了求得的最优解的PCS和原问题的最优解的PCS差多少 OCBA分配规则是说在每个step都各个design瓜分$\\Delta$时都采用定理一的准则，但其实定理一是在$\\Delta \\rightarrow \\infty$的时候才成立的，理论上这一点其实过不去，虽然可能实际效果很好；而且OCBA分配规则的最优性和收敛性好像都没有给出说明。 ","permalink":"https://huxiao09.github.io/blog/posts/paper/%E8%BF%90%E7%AD%B9%E4%BC%98%E5%8C%96-1/","summary":"Simulation Budget Allocation for Further Enhancing the Efficiency of Ordinal Optimization OCBA最出名的一篇文章。 文章概述 为了用尽量少的计算量得到尽可能高的对齐概率（挑选出最有可能是真实性能最好的desig","title":"【运筹优化 1】 OCBA"},{"content":"Optimizing Industrial HVAC Systems with Hierarchical Reinforcement Learning Deepmind新出的一篇关于数据中心制冷的文章。\n文章概述 使用HRL，上层智能体控制冷机的开关，下层智能体给出温度设定点，在仿真环境中达到了同时节能、防止过温、平衡冷机使用的目的。\n主要内容 Intro HVAC温室气体排放占10%，2016年数据。 RL常用来作为supervisory controller，确定各种设定点，把节能和温度约束弄成一个优化问题 工业HAVC设备不能频繁启停，不能一台老开着，不然会有磨损，这有一个最核心的问题是：单智能体难以reason across both extremely long and short time horizons 工业制冷要求：冷机只能每几个小时启停；各台冷机应该使用均匀；建筑的温度constraint HRL有reason across different time scales的能力，可以用来解决这个问题 Related Work 有很多工作使用机器学习、神经网络的方法构造冷机的模型（数据驱动），然后进行后续的优化 已经有一些RL算法应用在建筑制冷的研究了，但多智能体少，HRL还没有过，很少有研究除了节能和建筑温度还考虑冷机使用率的 Method 【MDP formulation】\n【Action】 每台冷机的启停、每台冷机的冷冻水出水温度设定点\n【State】 开启的冷机数、建筑温度、冷机能耗等\n【reward】 $\\mathbf{R}(t)=\\alpha_h \\cdot \\mathbf{h}(t)^{\\lambda_h}-\\alpha_o \\cdot \\mathbb{1}\\left(n_e \\neq n_d\\right)+\\alpha_p \\cdot \\mathbf{p}(t)^{\\lambda_p}-\\alpha_c \\cdot \\mathbf{c}(t)^{\\lambda_c}$\n其中，为正则化熵$\\mathbf{h}(t)$用于使各个冷机负载保持平衡，$\\mathbb{1}\\left(n_e \\neq n_d\\right)$为了防止智能体总是把所有冷机都全开或者全关，$\\mathbf{h}(t)$是能耗指标，$\\mathbf{c}(t)$是过温指标。注意这里的符合平衡惩罚的表达值得学习。\n【分层强化学习】\noption framework: 下层智能体用于学习和选择option，上层智能体用于提供终止条件（可以被视为option需要达到的目标）\n在本文的问题中，上层智能体控制冷机的开关，下层智能体给出温度设定点。在每一个时间步，上层智能体提供一个step goal即调用下层智能体的step数（时间）。\nExperiments 【环境搭建】\n用的是《Semi-analytical Industrial Cooling System Model for Reinforcement Learning》这篇文章的环境，应该是用COMSOL搭建出来的。一个仿真步平均20-40秒（不清楚是现实时间还是仿真时间），一个episode仿真环境12小时，建筑冷负荷由正弦模拟，湿球温度也具有正弦模拟的偏差。\n【结果】\n选了DMPO、DDPG、D4PG、HBP做对比。HRL学到的episode return比较高，而且比较安全，但是最后的episode return不如DMPO，作者把锅甩到了reward设计不能capture human preference上？😦然后又用real world feasibility来说明了HRL相比DMPO更不用频繁启停冷机。文中也多次指出HRL的方法不用细致设计reward，就能达到想要的目的。\nReviews 【优点】\nreward中多机平衡的熵表示值得学习 对于没有办法实际落地的算法，尽量写一个real world feasibility，可以统计在仿真环境中constraint violations的次数 HRL可能确实是一个思路，后续可以考虑 【缺点】\n一个最明显的缺点是只在一个仿真环境中跑了算法，仿真参数也都是自己给的，保证不了在实际场景中的性能 算法对比的时候选了DMPO、DDPG、D4PG、HBP，感觉还是不足，在实际使用场景应该有一些更简单的算法或者基于经验的算法，这里没拿出来比，也没有比TD3，而且为啥图3没有DDPG、D4PG的曲线？ 一些实验细节没说，好像没说一个仿真步具体在仿真环境和现实环境中的时间比，有些仿真器真的仿得很慢；没有考虑末端空调和室内的热交换，末端好像基本就当成了一个集总模型。 ","permalink":"https://huxiao09.github.io/blog/posts/paper/%E5%88%B6%E5%86%B7%E4%BC%98%E5%8C%96-1/","summary":"Optimizing Industrial HVAC Systems with Hierarchical Reinforcement Learning Deepmind新出的一篇关于数据中心制冷的文章。 文章概述 使用HRL，上层智能体控制冷机的开关，下层智能体给出温度设定点，在","title":"【制冷优化 1】 Optimizing Industrial HVAC Systems with Hierarchical Reinforcement Learning"},{"content":"建站原因 其实我一直以来都有写一系列博客记录自己科研学习和生活的想法，但是由于过去种种忙碌，这个想法一直没有付诸于行动。但是最近自己遭遇了一些非常难受的事情，也对自身进行了很多反思，还是决定从现在开始，要将之后在科研道路上的学习笔记、学习总结、对于生活的感悟写下来，希望自己能够坚持下去，等文章数多了就再买个域名，等过些年再回看这些博客，可能也会感慨自己这些年的成长。\n建站最直接的原因是暂时失去了一段很珍贵的感情，在我仅有的这23年春秋中，这算是给我打击非常非常大的一件事了。自己反思了很长时间，在这段感情中我确实有很多不足和做的不好的地方，也明白了很多道理，可能人确实需要这一些打击来不断革新自我吧。一些细节就不说了，这次事件使我明白的很重要的道理之一是：”人在爱别人之前，得先学会爱自己“。这句话好像有很多人说过，暂时不知道出处是哪里了，但是直到现在我可能才从自己的视角，明白这句话的涵义，这也是促使我决定开始写博客记录的原因。这句话其实并不是让人们在爱情中自私自利、只考虑自己，而是说我们得先把自己的生活处理好，有健康的体魄和人生观，有一两个愿意坚持的爱好，有几个能够交心的朋友，有自己一直以来坚定的理想，有打心底接纳自己并爱自己的自信，才能以更好地姿态去爱别人，去让别人感受自己的美好，去感受别人的美好，去发自内心地关心别人，就像关心自己一样。从本科到现在，我似乎因为保研、毕业等种种压力，很久没有停下来认真思考和感受生活了，我好像渐渐丧失了爱自己的能力，只顾着抓紧时间去卷来卷去，但却失去了认真生活的能力，对生活的理解出现了偏差，也导致自己在爱情中不够成熟，没有能真正切身站在对方角度考虑问题的能力，没有和对方一起考虑生活、享受生活的能力，因为我可能自己就压根不知道怎么生活。当然，失去这段感情的原因还有很多，但是这一条其实是我明白的一个很重要的道理，也是以后要改进的方向。因此，我还是决定以写博客的方式记录一下自己的科研总结、生活感悟和记录，也算是对自己的一种督促。卷是永远没有尽头的，适度努力，感受生活，关爱他人，关爱自己，多晒晒太阳🌞。\n考虑到有些科研总结和生活日常还是有些私密性的，主要用于自身的总结，所以正好可以用github.io或自己买的域名记录，可能只有少数熟悉自己的同学和朋友才会点进来看，也不会被百度检索到，挺好。\n博客内容 未来最主要记录的还是日常科研读的一些文章，主要是RL、运筹优化这些，还有对一些topic的总结，会穿插记录一些生活小事、对于生活的感悟和思考、健身阶段性的记录；另外，在此立一个flag，一定一定要把健身坚持下去！即使未来再忙，都应该注意自己的身体，坚持健身，定期体检。希望经历了这20天的休整和反思之后，能以一个更加健康的心态和体态，面对未来的生活，加油！\n","permalink":"https://huxiao09.github.io/blog/posts/life/2022_10_19/","summary":"建站原因 其实我一直以来都有写一系列博客记录自己科研学习和生活的想法，但是由于过去种种忙碌，这个想法一直没有付诸于行动。但是最近自己遭遇了一些","title":"建站第一篇"},{"content":"","permalink":"https://huxiao09.github.io/blog/posts/tech/tech/","summary":"","title":"TRPO"},{"content":"\rSulv\u0026#39;s Blog\r一个记录技术、阅读、生活的博客\r👉友链格式\r名称： Sulv\u0026rsquo;s Blog 网址： https://www.sulvblog.cn 图标： https://www.sulvblog.cn/img/Q.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求\r秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内\n👉Hugo博客交流群\r787018782\n","permalink":"https://huxiao09.github.io/blog/links/","summary":"Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： Sulv\u0026rsquo;s Blog 网址： https://www.sulvblog.cn 图标： https://www.sulvblog.cn/img/Q.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求 秉承互换友链原则、文","title":"🤝友链"},{"content":"个人简历：链接\n","permalink":"https://huxiao09.github.io/blog/about/","summary":"个人简历：链接","title":"🙋🏻‍♂️关于我"}]